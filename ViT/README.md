# Prescored Transformer - ViT

This folder contains the code to test our prescored method on Vision Transformer (ViT) attention mechanisms. The implementation evaluates the effectiveness of the prescored approach in improving attention performance.

## Features
- Testing framework for prescored method on ViT.
- Scripts for analyzing attention mechanisms.

## Usage
1. Clone the repository.
2. Navigate to this folder.
3. Run the provided scripts to test the prescored method.

## Requirements
- Python 3.x
- PyTorch
- Additional dependencies listed in `requirements.txt`.

# License
The code is licensed under the Apache 2.0 license.
