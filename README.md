# Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in Transformers

The repository is the PyTorch implementation of pre-scoreing attention paper on Hyperattention and ViT:

Paper is not published yet...

Details of implementation are in each folder.

# License
The code is licensed under the Apache 2.0 license.


# Citation

